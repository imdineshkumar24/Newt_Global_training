# -*- coding: utf-8 -*-
"""Copy of Unit 5Modelling_without_with_MLpipeline_updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HJd97KtABq7yzXmp2NK4ucjwpIb22CaS
"""

!pip install pyspark

from pyspark.sql import SparkSession

# Step 2: Create a Spark session
spark = SparkSession.builder.appName("Example").getOrCreate()

filepath="/content/houseprice.csv"
df=spark.read.format('csv').options(header='true',inferSchema='true',delimiter=';').load(filepath);
df.show(5,truncate=False)
df.printSchema()

print(type(df))

df1=spark.read.csv(filepath,header=True)
# slelct columns
df1.show()
print(type(df1))

df1.select("country","price").show(5)

from pyspark.sql import types as t
from pyspark.sql import functions as f

cdf1=df1.withColumn("price",f.when(f.col("price") > 600000 ,"high").otherwise("low"))
print(type(cdf1))

cdf1.show(5)
cdf1.select("country","price").show(5)
cdf1.groupBy("price").count().show()

from pyspark.sql.types import IntegerType,StringType

#train_df1 = cdf1.withColumn("price", train_df1["price"].cast(StringType()))
cdf1=cdf1.withColumn("bathrooms", cdf1["bathrooms"].cast(IntegerType()))
cdf1=cdf1.withColumn("bedrooms", cdf1["bedrooms"].cast(IntegerType()))
cdf1= cdf1.withColumn("sqft_living", cdf1["sqft_living"].cast(IntegerType()))
cdf1=cdf1.withColumn("sqft_lot",cdf1["sqft_lot"].cast(IntegerType()))
cdf1 = cdf1.withColumn("floors", cdf1["floors"].cast(IntegerType()))
cdf1=cdf1.withColumn("sqft_basement", cdf1["sqft_basement"].cast(IntegerType()))
cdf1.printSchema();

(train_df1, test_df1) = cdf1.randomSplit([0.8, 0.2], 11)
print("Number of train samples: " + str(train_df1.count()))
print("Number of test samples: " + str(test_df1.count()))

from pyspark.ml.feature import StringIndexer

price_indexer = StringIndexer(inputCol="price", outputCol="price_index")

price_indexed_df1 = price_indexer.fit(train_df1);
print(price_indexed_df1)

train_df1=price_indexed_df1.transform(train_df1);
train_df1.show();

from pyspark.ml.feature import VectorAssembler

# DecisionTreeClassifier is used for classiication problems
from pyspark.ml.classification import DecisionTreeClassifier

# Convert features into vector
inputCols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot','floors', 'sqft_basement']
outputCol = "features"
#create a VectorAssembler object
# to transform the specified input columns into a single feature vector column

vector_assembler = VectorAssembler(inputCols = inputCols, outputCol = outputCol)
train_df1 = vector_assembler.transform(train_df1)
train_df1.show(5)

# Select feature vector and label
modeling_df = train_df1.select(['features', 'price_index'])

# Create DecisionTreeClassifier model
dt_model = DecisionTreeClassifier(labelCol="price_index", featuresCol="features")

# Train model with Training Data
dt_model = dt_model.fit(modeling_df)

# Do predictions on train data
predictions = dt_model.transform(modeling_df)
predictions.show(20, truncate=False)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluatorDT = MulticlassClassificationEvaluator(labelCol="price_index")
area_under_curve = evaluatorDT.evaluate(predictions)

print(area_under_curve)

from pyspark.ml.feature import StringIndexer

price_indexer = StringIndexer(inputCol="price", outputCol="price_index")
price_indexed_df1 = price_indexer.fit(test_df1);
test_df1=price_indexed_df1.transform(test_df1);
test_df1.show();
test_df1.printSchema();

# On Test data - Transform test data using all the transformers and estimators in the same order
test_df1 = vector_assembler.transform(test_df1)
test_predictions = dt_model.transform(test_df1)
test_predictions.show(20, truncate=False)

area_under_curve = evaluatorDT.evaluate(test_predictions)
area_under_curve

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from sklearn.metrics import classification_report

# Calculate accuracy
evaluator = MulticlassClassificationEvaluator(labelCol="price", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(test_predictions)

# Convert the PySpark DataFrame to a Pandas DataFrame
test_predictions_pd = test_prediction.select("price", "prediction").toPandas()

# Get the true labels and predicted labels as lists
true_labels = test_predictions_pd["price"].tolist()
predicted_labels = test_predictions_pd["prediction"].tolist()

# Generate the classification report
report = classification_report(true_labels, predicted_labels)

# Print the classification report
print(report)